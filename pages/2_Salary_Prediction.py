import os

import streamlit as st
import joblib

from css_fro_streamlit import css
from models.model_2_architecture import Net
from prep_data_for_model import *
from transformers import AutoTokenizer, AutoModel

st.sidebar.page_link("streamlit_app.py", label="–ì–ª–∞–≤–Ω–∞—è")
st.sidebar.page_link("pages/1_Analytics.py", label="–ê–Ω–∞–ª–∏–∑ –∑–∞—Ä–ø–ª–∞—Ç")
st.sidebar.page_link("pages/2_Salary_Prediction.py", label="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã")
st.sidebar.page_link("pages/3_Useful_info.py", label="–ü–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")


@st.cache_resource
def get_mlb():
    return joblib.load('models/mlb_encoder.pkl')


@st.cache_resource
def get_tokenizer():
    try:
        tokenizer_ = AutoTokenizer.from_pretrained("./rubert_tokenizer2")
        return tokenizer_
    except:
        tokenizer_ = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")
        tokenizer_.save_pretrained("./rubert_tokenizer2")
        return tokenizer_


@st.cache_resource
def get_rubert_model():
    try:
        model_ = AutoModel.from_pretrained("./rubert_model")
        return model_
    except:
        model_ = AutoModel.from_pretrained("DeepPavlov/rubert-base-cased")
        model_.save_pretrained("./rubert_model")
        return model_


def predict_salary_torch(name, description, experience_str,
                   tokenizer, model_rubert, model_net, device,
                   y_train_mean=127050.85983732407, y_train_std=74488.1467090772
                   ):
    """
    name, description: —Å—Ç—Ä–æ–∫–∏
    experience_str: —Å—Ç—Ä–æ–∫–∞ –∏–∑ experience_mapping
    """
    # üëá –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç ‚Äî –∫–∞–∫ –≤ —Ç–≤–æ—ë–º –ø–∞–π–ø–ª–∞–π–Ω–µ
    input_text = build_input2(name, description)

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ ruBERT
    model_rubert.eval()
    with torch.no_grad():
        inputs = tokenizer([input_text], return_tensors="pt", truncation=True, padding=True, max_length=512)
        outputs = model_rubert(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().ravel()

    # –û–ø—ã—Ç —á–µ—Ä–µ–∑ mapping
    exp_val = experience_mapping.get(experience_str.strip(), 0)

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    input_features = np.concatenate([embedding, [exp_val]], axis=0).astype(np.float32)

    # –ü—Ä–æ–≥–æ–Ω —á–µ—Ä–µ–∑ —Å–µ—Ç—å
    model_net.eval()
    with torch.no_grad():
        x_tensor = torch.tensor(input_features, dtype=torch.float32).unsqueeze(0).to(device)
        pred_scaled = model_net(x_tensor).cpu().numpy().ravel()[0]
        pred_rub = pred_scaled * y_train_std + y_train_mean

    return pred_rub


# –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
models = {
    "–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å": "models/random_forest_model2.pkl",
    "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (v1)": "models/model_NN_1_full.pt",
    "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (v2)": "models/model_NN_2_full.pt"
}

# –ö–∞—Å—Ç–æ–º–Ω—ã–π CSS –¥–ª—è —Å—Ç–∏–ª–∏–∑–∞—Ü–∏–∏
st.markdown(css, unsafe_allow_html=True)

# st.title("UCARY")

# tab1, tab2 = st.tabs(["–ê–Ω–∞–ª–∏—Ç–∏–∫–∞", "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—èüîÆ"])
# with tab1:
#     st.header("–¢—É—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫–∞")
# with tab2:
# –•–µ–¥–µ—Ä
# st.header("–î–∞—à–±–æ—Ä–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Ä–ø–ª–∞—Ç—ã")
st.markdown('<div class="header">–î–∞—à–±–æ—Ä–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞—Ä–ø–ª–∞—Ç—ã</div>', unsafe_allow_html=True)

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
st.markdown('<div class="main-content">', unsafe_allow_html=True)

# –í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
st.subheader("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å")
selected_model = st.selectbox("–ú–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", list(models.keys()), label_visibility="collapsed")

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
# model = joblib.load(models[selected_model])
selected_model_path = models[selected_model]
_, ext = os.path.splitext(selected_model_path)



# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
if ext == ".pkl":
    model = joblib.load(selected_model_path)
elif ext == ".pt":
    model = Net(769)
    # model.load_state_dict(torch.load(selected_model_path, weights_only=True))
    model = torch.load(selected_model_path, weights_only=False)
    model.eval()
else:
    st.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏: {ext}")
    st.stop()

mlb = get_mlb()
tokenizer = get_tokenizer()
model_rubert = get_rubert_model()

# –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è –≤–≤–æ–¥–∞ URL
st.subheader("–í–≤–µ–¥–∏—Ç–µ URL –≤–∞–∫–∞–Ω—Å–∏–∏")

st.text("Url –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞:")
st.code("https://vladikavkaz.hh.ru/vacancy/116838770", language='text')
url = st.text_input("URL", placeholder="https://vladikavkaz.hh.ru/vacancy/id", label_visibility="collapsed")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ URL –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
if url:
    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏..."):
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
            api_url = convert_hh_url_to_api(url)
            vacancy_data = parse_vacancy(api_url)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã
            if ext == ".pkl":
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                features = prepare_features(vacancy_data, mlb, tokenizer, model_rubert)

                predicted_salary = model.predict(features)[0]
            elif ext == ".pt":
                predicted_salary = predict_salary_torch(
                    safe_get(vacancy_data, "name", ""),
                    safe_get(vacancy_data, "description", ""),
                    experience_mapping.get(safe_get(safe_get(vacancy_data, "experience", {}), "name", ""), 1),
                    tokenizer, model_rubert, model,
                    device=torch.device("cpu"),
                )

            delta = 30_000 // 2
            salary_from, salary_to = predicted_salary - delta, predicted_salary + delta

            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –î–µ—Ç–∞–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–∏")
            sf = safe_get(safe_get(vacancy_data, "salary", {}), "from", None)
            st_ = safe_get(safe_get(vacancy_data, "salary", {}), "to", None)
            original_salary_html = ""
            if sf is not None and st_ is not None:
                original_salary_html = f"{sf:,.0f} - {st_:,.0f}"

            elif sf is not None:
                original_salary_html = f"{sf:,.0f} - ?"

            elif st_ is not None:
                original_salary_html = f" ? - {st_:,.0f}"
            else:
                "–ù–µ —É–∫–∞–∑–∞–Ω"
            st.markdown(f"""
            <div class="vacancy-box">
                <strong>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–∞—Ä–ø–ª–∞—Ç—ã (RUB):</strong> {salary_from:,.0f} - {salary_to:,.0f}<br>
                <strong>–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–∞—Ä–ø–ª–∞—Ç—ã (RUB):</strong> {original_salary_html} <br>
                <br>
                <strong>–ù–∞–∑–≤–∞–Ω–∏–µ:</strong> {vacancy_data['name']}<br>
                <strong>–û–ø–∏—Å–∞–Ω–∏–µ:</strong> {vacancy_data['description']}<br>
            </div>""", unsafe_allow_html=True)

        except Exception as e:
            print(e)
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL: {str(e)}")
else:
    st.markdown(f"""
               <div class="vacancy-box">
                   <strong>–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–∞—Ä–ø–ª–∞—Ç—ã (RUB):</strong> –û–∂–∏–¥–∞–Ω–∏–µ    –≤–≤–æ–¥–∞... <br>
                   <strong>–ù–∞–∑–≤–∞–Ω–∏–µ:</strong> –í–≤–µ–¥–∏—Ç–µ    URL, —á—Ç–æ–±—ã    —É–≤–∏–¥–µ—Ç—å    –¥–µ—Ç–∞–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–∏<br>
                   <strong>–û–ø–∏—Å–∞–Ω–∏–µ:</strong> –û–∂–∏–¥–∞–Ω–∏–µ    –≤–≤–æ–¥–∞...<br>
               </div>""", unsafe_allow_html=True)

# –ó–∞–∫—Ä—ã—Ç–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
st.markdown('</div>', unsafe_allow_html=True)

# –§—É—Ç–µ—Ä
st.markdown("""
<div class ="footer">
    –°–æ–∑–¥–∞–Ω–æ —Å—Ç—É–¥–µ–Ω—Ç–∫–æ–π –º–∞—Ç—Ñ–∞–∫–∞ –°–û–ì–£ |
    <a href = "https://github.com/Lana-Dzuceva"> GitHub </a> |
    <a href = "https://t.me/Lana_hmm"> Telegram </a> <br>
    –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–µ xAI | –°–æ–∑–¥–∞–Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    Streamlit | –î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è
    —Å –ø–æ–º–æ—â—å—é DuckDB
    –∏ Random Forest
</div>
""", unsafe_allow_html=True)

# –ò–º–∏—Ç–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–∫–∏
# time.sleep(1)